# ER+/HER2- Breast Cancer Grade Classification Using Machine Learning
> Intepretable grade prediction pipeline for **ER+/HER2- Breast Cancer** using an **8-gene signature**. It includes classification, clustering, survival analysis, knowledge distillation, OCR-based grade extraction, and LLM-powered clinical reporting.

## ğŸ§¬ Overview

This repository implements an effective and interpretable pipeline for predicting histological tumor grade in ER+/HER2- breast cancer patients.  

Key components include:
- Feature selection and grade classification using METABRIC gene expression data.
- Clustering analysis on TCGA samples.
- Survival analysis and statistical validation.
- Integration of OCR-extracted clinical annotations from pathology reports.
- Model explainability using SHAP.
- Report generation using Large Language Models (LLMs) for both patients and clinicians.


## ğŸ“‚ Repository Structure

```plaintext
.
â”œâ”€â”€ LLM-generated-reports/
â”‚   â”œâ”€â”€ Gemini_2.0-Flash (reports and prompts)
â”‚   â”œâ”€â”€ GPT-4o (reports and prompts)
â”‚   â””â”€â”€ experts_evaluation.xlsx (full per-patient and per-dimension scores)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ METABRIC_classification.ipynb
â”‚   â”œâ”€â”€ TCGA_clustering_ER+.ipynb
â”‚   â””â”€â”€ metrics.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ METABRIC_analysis.R
â”‚   â”œâ”€â”€ TCGA_analysis.R
â”‚   â”œâ”€â”€ download_reports.py
â”‚   â””â”€â”€ reports_filtering.py
â”‚
â”œâ”€â”€ TCGA_OCR_grades.csv (OCR-extracted grade information for TCGA)
â””â”€â”€ README.md
```

## ğŸ“ OCR-Based Grade Extraction

To foster further research, grade information for TCGA patients â€“ extracted via OCR from unstructured, multipage PDF pathology reports â€“ is provided in the `TCGA_OCR_grades.csv` file. Used reports are publicly available online at this [link](https://github.com/inodb/datahub/tree/add-symlink-path-report/tcga/pathology_reports).

## ğŸ“Š Expert Evaluations of LLM-Generated Clinical Reports

Reports were manually evaluated by domain experts on a 0â€“5 Likert scale (0 = poor, 5 = excellent) across four criteria:
- *Clinical clarity* â€“ Is the interpretation clear, understandable, and clinically meaningful?
- *Scientific accuracy* â€“ Are the genetic contributors correctly explained in the context of breast cancer biology?
- *Clinical utility* â€“ Does the report support clinical decision-making?
- *Structural readability* â€“ Is the report logically organized and easy to read?

An *overall quality* score was also computed as the mean of these four. To reduce bias, LLMs were anonymized as *LLM A* and *LLM B* during evaluation. The actual model identities are disclosed in the file post-evaluation.

See [`experts_evaluation.xlsx`](./experts_evaluation.xlsx) for:
- Scores achieved per patient and criterion  
- Post-evaluation model mapping  


## âœ‰ï¸ Contact

For questions, collaborations, or support, feel free to contact:
- *Riccardo Cantini* â€“ [riccardo.cantini@unical.it](mailto:riccardo.cantini@unical.it)  
- *Marianna Talia* â€“ [marianna.talia@unical.it](mailto:marianna.talia@unical.it)
